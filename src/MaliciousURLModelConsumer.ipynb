{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05363ff8-ee11-44e4-9547-93e575345e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import tree\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import compose\n",
    "from river import naive_bayes\n",
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from confluent_kafka import Producer,Consumer\n",
    "import certifi\n",
    "import time\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9c06d881-59ee-432a-a205-72d7f4c064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user= os.environ['kafka_username']\n",
    "password= os.environ['kafka_password']\n",
    "bsts= os.environ['kafka_bootstrap_servers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "93333d02-5133-43a6-8ef2-17d25f3f0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic = 't_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781f351-b857-48e0-9bbb-9b222625d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e75d7bf0-296f-42f8-8848-f9f438284085",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[]\n",
    "classifier='HoeffdingAdaptiveTreeClassifier'\n",
    "classifiers.append(classifier)\n",
    "classifier='SRPClassifierHAT'\n",
    "classifiers.append(classifier)\n",
    "classifier='SRPClassifierNaiveBayes'\n",
    "classifiers.append(classifier)\n",
    "classifier='AdaptiveRandomForestClassifier'\n",
    "classifiers.append(classifier)\n",
    "classifier='HalfSpaceTrees'\n",
    "classifiers.append(classifier)\n",
    "\n",
    "model_artifact = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e99e5efa-2589-43d1-ab3f-ddfcde4b4925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Avg Time(ms)</th>\n",
       "      <th>Exp Train Time 1 million records (s)</th>\n",
       "      <th>Exp time 1 million (mins)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HoeffdingAdaptiveTreeClassifier</td>\n",
       "      <td>1.919338</td>\n",
       "      <td>1919.337511</td>\n",
       "      <td>31.988959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRPClassifierHAT</td>\n",
       "      <td>29.139747</td>\n",
       "      <td>29139.746666</td>\n",
       "      <td>485.662444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPClassifierNaiveBayes</td>\n",
       "      <td>4.398058</td>\n",
       "      <td>4398.058414</td>\n",
       "      <td>73.300974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaptiveRandomForestClassifier</td>\n",
       "      <td>1.029698</td>\n",
       "      <td>1029.698372</td>\n",
       "      <td>17.161640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HalfSpaceTrees</td>\n",
       "      <td>0.229605</td>\n",
       "      <td>229.604959</td>\n",
       "      <td>3.826749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Classifier  Avg Time(ms)  \\\n",
       "0  HoeffdingAdaptiveTreeClassifier      1.919338   \n",
       "1                 SRPClassifierHAT     29.139747   \n",
       "2          SRPClassifierNaiveBayes      4.398058   \n",
       "3   AdaptiveRandomForestClassifier      1.029698   \n",
       "4                   HalfSpaceTrees      0.229605   \n",
       "\n",
       "   Exp Train Time 1 million records (s)  Exp time 1 million (mins)  \n",
       "0                           1919.337511                  31.988959  \n",
       "1                          29139.746666                 485.662444  \n",
       "2                           4398.058414                  73.300974  \n",
       "3                           1029.698372                  17.161640  \n",
       "4                            229.604959                   3.826749  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_size=1000\n",
    "dataset = datasets.MaliciousURL()\n",
    "d=[]\n",
    "for classifier in classifiers:\n",
    "    data = dataset.take(max_size)\n",
    "    if classifier=='HoeffdingAdaptiveTreeClassifier':\n",
    "            model_artifact = tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10,seed=0)\n",
    "    elif classifier=='SRPClassifierHAT':\n",
    "            model_artifact =  ensemble.SRPClassifier(\n",
    "                                                   model=tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10, seed=0), seed=42,\n",
    "                                                )\n",
    "    elif classifier=='SRPClassifierNaiveBayes':\n",
    "            model_artifact = ensemble.SRPClassifier(\n",
    "                                                 model=naive_bayes.BernoulliNB(alpha=1), seed=42,\n",
    "                                              )\n",
    "    elif classifier=='AdaptiveRandomForestClassifier':\n",
    "            model_artifact = ensemble.AdaptiveRandomForestClassifier(leaf_prediction=\"mc\")\n",
    "\n",
    "    elif classifier=='HalfSpaceTrees':\n",
    "            model_artifact = compose.Pipeline(preprocessing.MinMaxScaler(),anomaly.HalfSpaceTrees(seed=42))\n",
    "    cnt = 0\n",
    "    training_st_ts = time.time()\n",
    "    for f, y in data:\n",
    "        cnt = cnt + 1\n",
    "        model_artifact = model_artifact.learn_one(f,y)\n",
    "    training_end_ts = time.time()\n",
    "    total_ts = training_end_ts-training_st_ts\n",
    "    avg_ts = (total_ts/max_size) * 1000    \n",
    "    d.append({'Classifier':classifier,\n",
    "             'Avg Time(ms)': (total_ts/max_size) * 1000,\n",
    "             'Exp Train Time 1 million records (s)':avg_ts * 1000,\n",
    "             'Exp time 1 million (mins)': avg_ts * (1000/60)})\n",
    "df = pandas.DataFrame(data=d)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be05c23-1ffc-4b27-b9ad-1be5078d7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "training_st_ts = time.time()\n",
    "for f, y in data:\n",
    "    cnt = cnt + 1\n",
    "    model_artifact = model_artifact.learn_one(f,y)\n",
    "training_end_ts = time.time()\n",
    "total_ts = training_end_ts-training_st_ts\n",
    "avg_ts = (total_ts/max_size) * 1000\n",
    "print(f'Training using no of records - {cnt}, total time taken in seconds- {total_ts} ')        \n",
    "print(f'Training using no of records - {cnt}, avg time per record in ms - {avg_ts} ')     \n",
    "print(f'Expected time to train using 1 million records {avg_ts * 1000} seconds or {avg_ts * 1000/60} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cb485-35d9-4bd6-bd49-23dd12b3ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records=1000000\n",
    "durations_per_partition = pandas.DataFrame()\n",
    "print(f'Total number of records - 1 million')\n",
    "no_of_partitions = range(10,201,10)\n",
    "d=[]\n",
    "for p in no_of_partitions:\n",
    "    d.append({'No of partitions':p,'Records per partition':round(total_records/p),'Training time(s)':(avg_ts * 1000/p),'Training time(mins)':(avg_ts * 1000/(60*p))})\n",
    "df = pandas.DataFrame(data=d)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a01db0e-39b6-44f5-af19-108145683fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using no of records - 10000, total time taken in seconds- 1.8953659534454346 \n",
      "Prediction using no of records - 10000, avg time per record in ms - 0.18953659534454345 \n",
      "Expected time to predict using 1 million records 189.53659534454346 seconds or 3.158943255742391 minutes\n"
     ]
    }
   ],
   "source": [
    "data = dataset.take(max_size)\n",
    "cnt = 0\n",
    "prediction_st_ts = time.time()\n",
    "for f, y in data:\n",
    "    cnt = cnt + 1\n",
    "    model_artifact.predict_one(f)\n",
    "prediction_end_ts = time.time()\n",
    "total_ts = prediction_end_ts-prediction_st_ts\n",
    "avg_ts = (total_ts/max_size) * 1000\n",
    "print(f'Prediction using no of records - {cnt}, total time taken in seconds- {total_ts} ')        \n",
    "print(f'Prediction using no of records - {cnt}, avg time per record in ms - {avg_ts} ')     \n",
    "print(f'Expected time to predict using 1 million records {avg_ts * 1000} seconds or {avg_ts * 1000/60} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4533c25-fb1c-4462-b944-978b3af8b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records - 1 million\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No of partitions</th>\n",
       "      <th>Records per partition</th>\n",
       "      <th>Prediction time(s)</th>\n",
       "      <th>Prediction time(mins)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>100000</td>\n",
       "      <td>18.953660</td>\n",
       "      <td>0.315894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>50000</td>\n",
       "      <td>9.476830</td>\n",
       "      <td>0.157947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>33333</td>\n",
       "      <td>6.317887</td>\n",
       "      <td>0.105298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>25000</td>\n",
       "      <td>4.738415</td>\n",
       "      <td>0.078974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>3.790732</td>\n",
       "      <td>0.063179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>16667</td>\n",
       "      <td>3.158943</td>\n",
       "      <td>0.052649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>14286</td>\n",
       "      <td>2.707666</td>\n",
       "      <td>0.045128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>12500</td>\n",
       "      <td>2.369207</td>\n",
       "      <td>0.039487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>11111</td>\n",
       "      <td>2.105962</td>\n",
       "      <td>0.035099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.895366</td>\n",
       "      <td>0.031589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>110</td>\n",
       "      <td>9091</td>\n",
       "      <td>1.723060</td>\n",
       "      <td>0.028718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>120</td>\n",
       "      <td>8333</td>\n",
       "      <td>1.579472</td>\n",
       "      <td>0.026325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>130</td>\n",
       "      <td>7692</td>\n",
       "      <td>1.457974</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>140</td>\n",
       "      <td>7143</td>\n",
       "      <td>1.353833</td>\n",
       "      <td>0.022564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>6667</td>\n",
       "      <td>1.263577</td>\n",
       "      <td>0.021060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>160</td>\n",
       "      <td>6250</td>\n",
       "      <td>1.184604</td>\n",
       "      <td>0.019743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170</td>\n",
       "      <td>5882</td>\n",
       "      <td>1.114921</td>\n",
       "      <td>0.018582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>5556</td>\n",
       "      <td>1.052981</td>\n",
       "      <td>0.017550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>5263</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>0.016626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.947683</td>\n",
       "      <td>0.015795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No of partitions  Records per partition  Prediction time(s)  \\\n",
       "0                 10                 100000           18.953660   \n",
       "1                 20                  50000            9.476830   \n",
       "2                 30                  33333            6.317887   \n",
       "3                 40                  25000            4.738415   \n",
       "4                 50                  20000            3.790732   \n",
       "5                 60                  16667            3.158943   \n",
       "6                 70                  14286            2.707666   \n",
       "7                 80                  12500            2.369207   \n",
       "8                 90                  11111            2.105962   \n",
       "9                100                  10000            1.895366   \n",
       "10               110                   9091            1.723060   \n",
       "11               120                   8333            1.579472   \n",
       "12               130                   7692            1.457974   \n",
       "13               140                   7143            1.353833   \n",
       "14               150                   6667            1.263577   \n",
       "15               160                   6250            1.184604   \n",
       "16               170                   5882            1.114921   \n",
       "17               180                   5556            1.052981   \n",
       "18               190                   5263            0.997561   \n",
       "19               200                   5000            0.947683   \n",
       "\n",
       "    Prediction time(mins)  \n",
       "0                0.315894  \n",
       "1                0.157947  \n",
       "2                0.105298  \n",
       "3                0.078974  \n",
       "4                0.063179  \n",
       "5                0.052649  \n",
       "6                0.045128  \n",
       "7                0.039487  \n",
       "8                0.035099  \n",
       "9                0.031589  \n",
       "10               0.028718  \n",
       "11               0.026325  \n",
       "12               0.024300  \n",
       "13               0.022564  \n",
       "14               0.021060  \n",
       "15               0.019743  \n",
       "16               0.018582  \n",
       "17               0.017550  \n",
       "18               0.016626  \n",
       "19               0.015795  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_records=1000000\n",
    "durations_per_partition = pandas.DataFrame()\n",
    "print(f'Total number of records - 1 million')\n",
    "no_of_partitions = range(10,201,10)\n",
    "d=[]\n",
    "for p in no_of_partitions:\n",
    "    d.append({'No of partitions':p,'Records per partition':round(total_records/p),'Prediction time(s)':(avg_ts * 1000/p),'Prediction time(mins)':(avg_ts * 1000/(60*p))})\n",
    "df = pandas.DataFrame(data=d)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84d1129-5f3e-4c31-b93b-4e7901c8195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages(group_id,model,only_predict=False):\n",
    "    global model_artifact\n",
    "    features_consumer_conf = {'bootstrap.servers': bsts,\n",
    "                          'sasl.username': user,\n",
    "                          'sasl.password': password,\n",
    "                          'sasl.mechanism': 'PLAIN',\n",
    "                          'security.protocol': 'SASL_SSL',\n",
    "                          'ssl.ca.location': certifi.where(),\n",
    "                          'group.id': group_id,\n",
    "                          'enable.auto.commit': True,\n",
    "                          'auto.commit.interval.ms':1000,         \n",
    "                          'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)  \n",
    "    \n",
    "    print(f'\\nNow subscribing to features topic:{feature_topic}')\n",
    "        \n",
    "    features_consumer.subscribe([feature_topic])\n",
    "    cnt = 0\n",
    "    msg = None\n",
    "    error_cnt = 0\n",
    "    end_learn_ts = 0\n",
    "    st_learn_ts = 0\n",
    "\n",
    "    st_processing_time = 0\n",
    "    \n",
    "    learning_durations=[]\n",
    "    prediction_durations=[]\n",
    "    processing_durations = []\n",
    "    score_and_truth = []\n",
    "    mem_usage = []\n",
    "    while(True):           \n",
    "        msg = features_consumer.poll(timeout=0.1)    \n",
    "        if msg is None: continue\n",
    "        if msg.error():\n",
    "            error_cnt = error_cnt + 1\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:                    \n",
    "                    if(error_cnt%1000==0):\n",
    "                        print('error')\n",
    "                        print(msg)\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                             (msg.topic(), msg.partition(), msg.offset()))\n",
    "        else:       \n",
    "            try:         \n",
    "                msg_arrival_time = time.time()\n",
    "                message = json.loads(msg.value().decode(\"utf-8\"))            \n",
    "                cnt = cnt + 1\n",
    "                \n",
    "                f = message['f']\n",
    "                y = (message['y']=='true')              \n",
    "                if(cnt==1):\n",
    "                    st_processing_time = time.time()\n",
    "                \n",
    "                st_prediction_time = time.time()            \n",
    "                score = model_artifact.predict_one(f)\n",
    "                score_and_truth.append({'y':y,'score':score})\n",
    "                end_prediction_time = time.time()  \n",
    "                prediction_durations.append(end_prediction_time-st_prediction_time)\n",
    "                \n",
    "                if not only_predict:\n",
    "                    st_learn_ts = time.time()\n",
    "                    model_artifact = model_artifact.learn_one(f,y)      \n",
    "                    end_learn_ts = time.time()\n",
    "                    learning_durations.append(end_learn_ts-st_learn_ts)            \n",
    "                \n",
    "                msg_departure_time = time.time()\n",
    "                processing_durations.append(msg_departure_time-msg_arrival_time)\n",
    "                if(cnt%10==0):\n",
    "                    mem_usage.append(model_artifact._raw_memory_usage)\n",
    "            except Exception as  e:      \n",
    "                print(json.loads(msg.value().decode(\"utf-8\")))\n",
    "                print(e, file=sys.stdout)\n",
    "                ignored = ignored + 1\n",
    "                print(f'ignored ={ignored} total = {cnt}')\n",
    "\n",
    "    print('CLOSING')\n",
    "    features_consumer.commit()\n",
    "    features_consumer.close() \n",
    "    total_time = time.time() - st_processing_time\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30613b73-7440-4d2d-bfdf-792563642547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from river import metrics\n",
    "\n",
    "def print_time_results(durations, type_of_duration):\n",
    "    if len(durations)==0:\n",
    "        return\n",
    "    mean = statistics.mean(durations)\n",
    "    median = statistics.median(durations)\n",
    "    max_dur = max(durations)\n",
    "    min_dur = min(durations)  \n",
    "    print(f'Type of durations : {type_of_duration} ' )\n",
    "    print(f'\\tAVG : {mean}')\n",
    "    print(f'\\MEDIAN : {median}')\n",
    "    print(f'\\MAX : {max_dur}')\n",
    "    print(f'\\MIN : {min_dur}')\n",
    "    \n",
    "def print_results(score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time):\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    for m in score:\n",
    "        y = m['y']\n",
    "        score = m['score']\n",
    "        auc = auc.update(y,score)\n",
    "        f1 = f1.update(y, score)\n",
    "        recall = recall.update(y, score)\n",
    "    \n",
    "  \n",
    "    total_records = len(durations)\n",
    "    avg_memory_usage = statistics.mean(mem_usage)\n",
    "    print(f'Messages consumed:{total_records},Total Cumulative Time: {total_time}')    \n",
    "    print(f'AUC{auc}')\n",
    "    print(f'F1 {f1}')\n",
    "    print(f'RECALL {recall}')\n",
    "    print(f'AVERAGE MEMORY USAGE {avg_memory_usage}')\n",
    "    print_time_results(processing_durations,f'PROCESSING DURATIONS FOR {processing_durations}')\n",
    "    print_time_results(prediction_durations,f'PREDICTION DURATIONS FOR {prediction_durations}')\n",
    "    print_time_results(prediction_durations,f'LEARNING DURATIONS FOR {learning_durations}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444fa65-9fe2-4c14-9134-0974a3c3e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now subscribing to features topic:test_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "group_id = 'ARFC_8'\n",
    "only_predict=False\n",
    "score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time = consume_messages(group_id,model,only_predict)\n",
    "print_results(score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d489dcf-8cd5-4a4e-9ddc-2cd133a67886",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_predict=True\n",
    "score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time = consume_messages(group_id,model,only_predict)\n",
    "print_results(score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
