{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05363ff8-ee11-44e4-9547-93e575345e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import tree\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import compose\n",
    "from river import naive_bayes\n",
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from confluent_kafka import Producer,Consumer,TopicPartition\n",
    "import certifi\n",
    "import time\n",
    "import json\n",
    "import pandas\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93333d02-5133-43a6-8ef2-17d25f3f0b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c06d881-59ee-432a-a205-72d7f4c064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user= os.environ['kafka_username']\n",
    "password= os.environ['kafka_password']\n",
    "bsts= os.environ['kafka_bootstrap_servers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f84d1129-5f3e-4c31-b93b-4e7901c8195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consume_messages(group_id,model_artifact,max_messages,feature_topic,partitions):    \n",
    "    features_consumer_conf = {'bootstrap.servers': bsts,\n",
    "                          'sasl.username': user,\n",
    "                          'sasl.password': password,\n",
    "                          'sasl.mechanism': 'PLAIN',\n",
    "                          'security.protocol': 'SASL_SSL',\n",
    "                          'ssl.ca.location': certifi.where(),\n",
    "                          'group.id': group_id,\n",
    "                          'enable.auto.commit': True,\n",
    "                          'auto.commit.interval.ms':1000,         \n",
    "                          'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)  \n",
    "    \n",
    "    print(f'\\nNow subscribing to features topic:{feature_topic}')\n",
    "        \n",
    "    #features_consumer.subscribe([feature_topic])\n",
    "    tps = []\n",
    "    for p in range(partitions):\n",
    "        tps.append(TopicPartition(feature_topic,p))\n",
    "    \n",
    "    features_consumer.assign(tps)\n",
    "    \n",
    "    cnt = 0\n",
    "    ignored=0\n",
    "    msg = None\n",
    "    error_cnt = 0\n",
    "    end_learn_ts = 0\n",
    "    st_learn_ts = 0\n",
    "\n",
    "    st_processing_time = 0\n",
    "    \n",
    "    learning_durations=[]\n",
    "    prediction_durations=[]\n",
    "    processing_durations = []\n",
    "    score_and_truth = []\n",
    "    mem_usage = []\n",
    "    end_to_end_processing_durations = []\n",
    "    messaging_latencies = []\n",
    "    while(True):           \n",
    "        if(cnt>0 and cnt%max_messages==0):\n",
    "            break;\n",
    "        messages = features_consumer.consume(num_messages=1000,timeout=0.1)    \n",
    "        if len(messages)==0: continue\n",
    "        \n",
    "        for msg in messages:\n",
    "            if msg is None: continue\n",
    "            if msg.error():\n",
    "                error_cnt = error_cnt + 1\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:                    \n",
    "                        if(error_cnt%1000==0):\n",
    "                            print('error')\n",
    "                            print(msg)\n",
    "                        sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            else:       \n",
    "                try:         \n",
    "                    msg_arrival_time = time.time()\n",
    "                    message = json.loads(msg.value().decode(\"utf-8\"))            \n",
    "                    cnt = cnt + 1\n",
    "\n",
    "                    f = message['f']\n",
    "                    y = (message['y']=='true')              \n",
    "                    msg_produce_ts = message['st']\n",
    "                    messaging_latencies.append(msg_arrival_time-msg_produce_ts)\n",
    "                    if(cnt==1):\n",
    "                        st_processing_time = time.time()\n",
    "\n",
    "                    st_learn_ts = time.time()\n",
    "                    model_artifact = model_artifact.learn_one(f,y)      \n",
    "                    end_learn_ts = time.time()\n",
    "                    learning_durations.append(end_learn_ts-st_learn_ts)            \n",
    "                        \n",
    "                    st_prediction_time = time.time()            \n",
    "                    score = model_artifact.predict_one(f)\n",
    "                    score_and_truth.append({'y':y,'score':score})\n",
    "                    end_prediction_time = time.time()  \n",
    "                    prediction_durations.append(end_prediction_time-st_prediction_time)\n",
    "\n",
    "\n",
    "                    msg_departure_time = time.time()\n",
    "                    processing_durations.append(msg_departure_time-msg_arrival_time)\n",
    "                    end_to_end_processing_durations.append(msg_departure_time-msg_produce_ts)\n",
    "                    if(cnt%1000==0):\n",
    "                        print(f'Processed {cnt}')\n",
    "                        mem_usage.append(model_artifact._raw_memory_usage)\n",
    "                    if(cnt%max_messages==0):\n",
    "                        break;\n",
    "                except Exception as  e:      \n",
    "                    if(cnt%max_messages==0):\n",
    "                        break;\n",
    "                    #print(json.loads(msg.value().decode(\"utf-8\")))\n",
    "                    #print(e, file=sys.stdout)\n",
    "                    ignored = ignored + 1\n",
    "                    if(ignored%100==0):\n",
    "                        print(e, file=sys.stdout)\n",
    "                        print(f'ignored ={ignored} total = {cnt}')\n",
    "\n",
    "    print('Closing Consumer')\n",
    "    features_consumer.commit()\n",
    "    features_consumer.close() \n",
    "    total_time = time.time() - st_processing_time\n",
    "    return score_and_truth,messaging_latencies,processing_durations,end_to_end_processing_durations, prediction_durations, learning_durations,mem_usage,total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30613b73-7440-4d2d-bfdf-792563642547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from river import metrics\n",
    "\n",
    "def get_record(type_of_duration,durations):\n",
    "\n",
    "   return {'Type of durations' :type_of_duration,\n",
    "        'MEAN':statistics.mean(durations)*1000,\n",
    "        'MEDIAN':statistics.median(durations)*1000,\n",
    "        'MAXIMUM':max(durations)*1000,\n",
    "        'MINIMUM':min(durations)*1000\n",
    "        }\n",
    "    \n",
    "def get_results(score_and_truth,messaging_latencies,processing_durations,end_to_end_processing_durations, prediction_durations, learning_durations,mem_usage,total_time):\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    for m in score_and_truth:\n",
    "        y = m['y']\n",
    "        score = m['score']\n",
    "        auc = auc.update(y,score)\n",
    "        f1 = f1.update(y, score)\n",
    "        recall = recall.update(y, score)\n",
    "    \n",
    "  \n",
    "    total_records = len(score_and_truth)\n",
    "    avg_memory_usage = statistics.mean(mem_usage)\n",
    "    print(f'Messages consumed:{total_records},Total Cumulative Time: {total_time}')    \n",
    "    print(f'{auc}')\n",
    "    print(f'{f1}')\n",
    "    print(f'{recall}')\n",
    "    print(f'AVERAGE MEMORY USAGE {avg_memory_usage}')\n",
    "    \n",
    "    d=[]\n",
    " \n",
    "   \n",
    "    d.append(get_record('MESSAGING LATENCY',messaging_latencies))\n",
    "    d.append(get_record('PROCESSING DURATIONS ON ARRIVAL',processing_durations))\n",
    "    d.append(get_record('PREDICTION DURATIONS ON ARRIVAL',prediction_durations))\n",
    "    d.append(get_record('LEARNING DURATIONS ON ARRIVAL',learning_durations))\n",
    "    d.append(get_record('END TO END DURATIONS',end_to_end_processing_durations))\n",
    "                          \n",
    "    df = pandas.DataFrame(data=d)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6444fa65-9fe2-4c14-9134-0974a3c3e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now subscribing to features topic:features_v4\n",
      "Processed 1000\n",
      "Processed 2000\n",
      "Processed 3000\n",
      "Processed 4000\n",
      "Processed 5000\n",
      "Closing Consumer\n"
     ]
    }
   ],
   "source": [
    "model_artifact = ensemble.AdaptiveRandomForestClassifier(leaf_prediction=\"mc\")\n",
    "group_id = 'ADRF_1'\n",
    "max_messages = 5000\n",
    "feature_topic = 'features_v4'\n",
    "no_of_partitions=8\n",
    "\n",
    "score_and_truth,messaging_latencies,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time = consume_messages(group_id,model_artifact,max_messages,feature_topic,no_of_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50259b8f-4e9c-4199-9dac-3f31cc357bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages consumed:5000,Total Cumulative Time: 11.676496505737305\n",
      "ROCAUC: 86.98%\n",
      "F1: 85.82%\n",
      "MicroRecall: 86.98%\n",
      "AVERAGE MEMORY USAGE 4123386.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type of durations</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>MEDIAN</th>\n",
       "      <th>MAXIMUM</th>\n",
       "      <th>MINIMUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MESSAGING LATENCY</td>\n",
       "      <td>699.240449</td>\n",
       "      <td>702.050090</td>\n",
       "      <td>1160.474300</td>\n",
       "      <td>140.311956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROCESSING DURATIONS ON ARRIVAL</td>\n",
       "      <td>1.163084</td>\n",
       "      <td>1.115799</td>\n",
       "      <td>52.106380</td>\n",
       "      <td>0.426054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PREDICTION DURATIONS ON ARRIVAL</td>\n",
       "      <td>0.155105</td>\n",
       "      <td>0.148773</td>\n",
       "      <td>0.520468</td>\n",
       "      <td>0.081062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEARNING DURATIONS ON ARRIVAL</td>\n",
       "      <td>0.968696</td>\n",
       "      <td>0.921965</td>\n",
       "      <td>51.896095</td>\n",
       "      <td>0.281096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>END TO END DURATIONS</td>\n",
       "      <td>700.403534</td>\n",
       "      <td>703.067303</td>\n",
       "      <td>1161.363602</td>\n",
       "      <td>141.811848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Type of durations        MEAN      MEDIAN      MAXIMUM  \\\n",
       "0                MESSAGING LATENCY  699.240449  702.050090  1160.474300   \n",
       "1  PROCESSING DURATIONS ON ARRIVAL    1.163084    1.115799    52.106380   \n",
       "2  PREDICTION DURATIONS ON ARRIVAL    0.155105    0.148773     0.520468   \n",
       "3    LEARNING DURATIONS ON ARRIVAL    0.968696    0.921965    51.896095   \n",
       "4             END TO END DURATIONS  700.403534  703.067303  1161.363602   \n",
       "\n",
       "      MINIMUM  \n",
       "0  140.311956  \n",
       "1    0.426054  \n",
       "2    0.081062  \n",
       "3    0.281096  \n",
       "4  141.811848  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_results(score_and_truth,messaging_latencies,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b747e2b-4839-4acb-bc93-da1df19f2b2c",
   "metadata": {},
   "source": [
    "## Types of Durations\n",
    "\n",
    "Note how the messaging latency accounts for most of the latency. A faster and more expensive cluster will improve throughput proportionately\n",
    "\n",
    "|    | Type of durations               |        MEAN |      MEDIAN |     MAXIMUM |     MINIMUM |\n",
    "|---:|:--------------------------------|------------:|------------:|------------:|------------:|\n",
    "|  0 | MESSAGING LATENCY               | 0.736597    | 0.719714    | 1.32849     | 0.226969    |\n",
    "|  1 | PROCESSING DURATIONS ON ARRIVAL | 0.00128025  | 0.00121534  | 0.0514345   | 0.000418901 |\n",
    "|  2 | PREDICTION DURATIONS ON ARRIVAL | 0.000170538 | 0.000166655 | 0.000534058 | 8.96454e-05 |\n",
    "|  3 | LEARNING DURATIONS ON ARRIVAL   | 0.00107017  | 0.00100136  | 0.0511785   | 0.000290155 |\n",
    "|  4 | END TO END DURATIONS            | 0.737877    | 0.720631    | 1.32997     | 0.228834    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29167097-33b1-4fd3-9be5-4edd90a90eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
