{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05363ff8-ee11-44e4-9547-93e575345e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import tree\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import compose\n",
    "from river import naive_bayes\n",
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from confluent_kafka import Producer,Consumer\n",
    "import certifi\n",
    "import time\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c06d881-59ee-432a-a205-72d7f4c064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user= os.environ['kafka_username']\n",
    "password= os.environ['kafka_password']\n",
    "bsts= os.environ['kafka_bootstrap_servers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93333d02-5133-43a6-8ef2-17d25f3f0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic = 'features_v1'\n",
    "model = compose.Pipeline(preprocessing.MinMaxScaler(),anomaly.HalfSpaceTrees(seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84d1129-5f3e-4c31-b93b-4e7901c8195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages(group_id,model,only_predict=False):    \n",
    "    features_consumer_conf = {'bootstrap.servers': bsts,\n",
    "                          'sasl.username': user,\n",
    "                          'sasl.password': password,\n",
    "                          'sasl.mechanism': 'PLAIN',\n",
    "                          'security.protocol': 'SASL_SSL',\n",
    "                          'ssl.ca.location': certifi.where(),\n",
    "                          'group.id': group_id,\n",
    "                          'enable.auto.commit': True,\n",
    "                          'auto.commit.interval.ms':1000,         \n",
    "                          'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)  \n",
    "    \n",
    "    print(f'\\nNow subscribing to features topic:{feature_topic}')\n",
    "        \n",
    "    features_consumer.subscribe([feature_topic])\n",
    "    cnt = 0\n",
    "    msg = None\n",
    "    error_cnt = 0\n",
    "    end_learn_ts = 0\n",
    "    st_learn_ts = 0\n",
    "\n",
    "    st_processing_time = 0\n",
    "    \n",
    "    learning_durations=[]\n",
    "    prediction_durations=[]\n",
    "    processing_durations = []\n",
    "    score_and_truth = []\n",
    "    mem_usage = []\n",
    "    end_to_end_processing_durations = []\n",
    "    while(True):           \n",
    "        messages = features_consumer.consume(num_messages=10000,timeout=0.1)    \n",
    "        if len(messages)==0: continue\n",
    "        for msg in messages:\n",
    "            if msg is None: continue\n",
    "            if msg.error():\n",
    "                error_cnt = error_cnt + 1\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:                    \n",
    "                        if(error_cnt%1000==0):\n",
    "                            print('error')\n",
    "                            print(msg)\n",
    "                        sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            else:       \n",
    "                try:         \n",
    "                    msg_arrival_time = time.time()\n",
    "                    message = json.loads(msg.value().decode(\"utf-8\"))            \n",
    "                    cnt = cnt + 1\n",
    "\n",
    "                    f = message['f']\n",
    "                    y = (message['y']=='true')              \n",
    "                    msg_produce_ts = message['st']\n",
    "                    if(cnt==1):\n",
    "                        st_processing_time = time.time()\n",
    "\n",
    "                    st_prediction_time = time.time()            \n",
    "                    score = model_artifact.predict_one(f)\n",
    "                    score_and_truth.append({'y':y,'score':score})\n",
    "                    end_prediction_time = time.time()  \n",
    "                    prediction_durations.append(end_prediction_time-st_prediction_time)\n",
    "\n",
    "                    if not only_predict:\n",
    "                        st_learn_ts = time.time()\n",
    "                        model_artifact = model_artifact.learn_one(f,y)      \n",
    "                        end_learn_ts = time.time()\n",
    "                        learning_durations.append(end_learn_ts-st_learn_ts)            \n",
    "\n",
    "                    msg_departure_time = time.time()\n",
    "                    processing_durations.append(msg_departure_time-msg_arrival_time)\n",
    "                    end_to_end_processing_durations.append(msg_departure_time-msg_produce_ts)\n",
    "                    if(cnt%100==0):\n",
    "                        mem_usage.append(model_artifact._raw_memory_usage)\n",
    "                except Exception as  e:      \n",
    "                    print(json.loads(msg.value().decode(\"utf-8\")))\n",
    "                    print(e, file=sys.stdout)\n",
    "                    ignored = ignored + 1\n",
    "                    print(f'ignored ={ignored} total = {cnt}')\n",
    "\n",
    "    print('CLOSING')\n",
    "    features_consumer.commit()\n",
    "    features_consumer.close() \n",
    "    total_time = time.time() - st_processing_time\n",
    "    return score_and_truth,processing_durations,end_to_end_processing_durations, prediction_durations, learning_durations,mem_usage,total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30613b73-7440-4d2d-bfdf-792563642547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from river import metrics\n",
    "\n",
    "def print_time_results(durations, type_of_duration):\n",
    "    if len(durations)==0:\n",
    "        return\n",
    "    mean = statistics.mean(durations)\n",
    "    median = statistics.median(durations)\n",
    "    max_dur = max(durations)\n",
    "    min_dur = min(durations)  \n",
    "    print(f'Type of durations : {type_of_duration} ' )\n",
    "    print(f'\\tAVG : {mean}')\n",
    "    print(f'\\MEDIAN : {median}')\n",
    "    print(f'\\MAX : {max_dur}')\n",
    "    print(f'\\MIN : {min_dur}')\n",
    "    \n",
    "def print_results(score_and_truth,processing_durations,end_to_end_processing_durations, prediction_durations, learning_durations,mem_usage,total_time):\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    for m in score:\n",
    "        y = m['y']\n",
    "        score = m['score']\n",
    "        auc = auc.update(y,score)\n",
    "        f1 = f1.update(y, score)\n",
    "        recall = recall.update(y, score)\n",
    "    \n",
    "  \n",
    "    total_records = len(durations)\n",
    "    avg_memory_usage = statistics.mean(mem_usage)\n",
    "    print(f'Messages consumed:{total_records},Total Cumulative Time: {total_time}')    \n",
    "    print(f'AUC{auc}')\n",
    "    print(f'F1 {f1}')\n",
    "    print(f'RECALL {recall}')\n",
    "    print(f'AVERAGE MEMORY USAGE {avg_memory_usage}')\n",
    "    print_time_results(processing_durations,f'PROCESSING DURATIONS FOR {processing_durations}')\n",
    "    print_time_results(prediction_durations,f'PREDICTION DURATIONS FOR {prediction_durations}')\n",
    "    print_time_results(prediction_durations,f'LEARNING DURATIONS FOR {learning_durations}')\n",
    "    print_time_results(processing_durations,f'END TO END PROCESSING DURATIONS FOR {processing_durations}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6444fa65-9fe2-4c14-9134-0974a3c3e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now subscribing to features topic:features_v1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m group_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHSFT_1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m only_predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m score_and_truth,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43monly_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m print_results(score_and_truth,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time)\n",
      "Cell \u001b[0;32mIn [13], line 32\u001b[0m, in \u001b[0;36mconsume_messages\u001b[0;34m(group_id, model, only_predict)\u001b[0m\n\u001b[1;32m     30\u001b[0m end_to_end_processing_durations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):           \n\u001b[0;32m---> 32\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures_consumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(messages)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "group_id = 'HSFT_1'\n",
    "only_predict=False\n",
    "score_and_truth,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time = consume_messages(group_id,model,only_predict)\n",
    "print_results(score_and_truth,processing_durations, end_to_end_processing_durations,prediction_durations, learning_durations,mem_usage,total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d489dcf-8cd5-4a4e-9ddc-2cd133a67886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
