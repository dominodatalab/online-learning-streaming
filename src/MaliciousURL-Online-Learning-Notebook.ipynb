{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05363ff8-ee11-44e4-9547-93e575345e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from river import datasets\n",
    "from river import ensemble\n",
    "from confluent_kafka import Producer,Consumer\n",
    "import certifi\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c06d881-59ee-432a-a205-72d7f4c064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user= os.environ['kafka_username']\n",
    "password= os.environ['kafka_password']\n",
    "bsts= os.environ['kafka_bootstrap_servers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93333d02-5133-43a6-8ef2-17d25f3f0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic = 'test_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4acf09b-509e-4e4c-a348-715e464e3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': bsts,\n",
    "            'sasl.mechanism': 'PLAIN',\n",
    "            'security.protocol': 'SASL_SSL',\n",
    "            'ssl.ca.location': certifi.where(),\n",
    "            'sasl.username': user,\n",
    "            'sasl.password': password,\n",
    "            'batch.num.messages': 2048,\n",
    "            #'queue.buffering.max.messages': 100,\n",
    "            'linger.ms': 100,\n",
    "            'client.id': 'producer-icde-2023'}\n",
    "producer = Producer(conf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781f351-b857-48e0-9bbb-9b222625d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size=4096\n",
    "dataset = datasets.MaliciousURL()\n",
    "data = dataset.take(max_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be05c23-1ffc-4b27-b9ad-1be5078d7e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz (233.66 MB)\n",
      "Uncompressing into /home/ubuntu/river_data/MaliciousURL\n",
      "flushing count - 128, time taken in seconds- 8.735549211502075 \n",
      "flushing count - 256, time taken in seconds- 0.009711265563964844 \n",
      "flushing count - 384, time taken in seconds- 0.009943246841430664 \n",
      "flushing count - 512, time taken in seconds- 0.009957313537597656 \n",
      "flushing count - 640, time taken in seconds- 0.009808540344238281 \n",
      "flushing count - 768, time taken in seconds- 0.010274887084960938 \n",
      "flushing count - 896, time taken in seconds- 0.010121345520019531 \n",
      "flushing count - 1024, time taken in seconds- 0.010085821151733398 \n",
      "flushing count - 1152, time taken in seconds- 0.009898900985717773 \n",
      "flushing count - 1280, time taken in seconds- 0.01019430160522461 \n",
      "flushing count - 1408, time taken in seconds- 0.01014566421508789 \n",
      "flushing count - 1536, time taken in seconds- 0.009747743606567383 \n",
      "flushing count - 1664, time taken in seconds- 0.009867429733276367 \n",
      "flushing count - 1792, time taken in seconds- 0.009691715240478516 \n",
      "flushing count - 1920, time taken in seconds- 0.009771585464477539 \n",
      "flushing count - 2048, time taken in seconds- 0.009891271591186523 \n",
      "final flushing count - 2048, time taken in seconds- 28.987449645996094 \n"
     ]
    }
   ],
   "source": [
    "end=0\n",
    "cnt = 0\n",
    "st = time.time()\n",
    "abs_st = time.time()\n",
    "for f, y in data:\n",
    "    cnt = cnt + 1    \n",
    "    d = {}\n",
    "    d['f']=f\n",
    "    d['y']=str(y).lower()\n",
    "    d['st']=time.time()  \n",
    "            \n",
    "    v= json.dumps(d).encode('utf-8')\n",
    "    try:\n",
    "        producer.produce(feature_topic, value=v, key=str(cnt))\n",
    "    except:\n",
    "      print(f'Queue full, flushing {cnt}')\n",
    "      producer.flush()\n",
    "      producer.produce(topic, value=v, key=str(cnt))\n",
    "    if cnt%1024==0:           \n",
    "        end = time.time()\n",
    "        print(f'flushing count - {cnt}, time taken in seconds- {end-st} ')        \n",
    "        producer.flush()\n",
    "        time.sleep(1)\n",
    "        st = time.time()        \n",
    "        \n",
    "producer.flush()\n",
    "end = time.time()\n",
    "print(f'final flushing count - {cnt}, time taken in seconds- {end-abs_st} ')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84d1129-5f3e-4c31-b93b-4e7901c8195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages(group_id,model):\n",
    "    features_consumer_conf = {'bootstrap.servers': bsts,\n",
    "                          'sasl.username': user,\n",
    "                          'sasl.password': password,\n",
    "                          'sasl.mechanism': 'PLAIN',\n",
    "                          'security.protocol': 'SASL_SSL',\n",
    "                          'ssl.ca.location': certifi.where(),\n",
    "                          'group.id': group_id,\n",
    "                          'enable.auto.commit': True,\n",
    "                          'auto.commit.interval.ms':1000,         \n",
    "                          'auto.offset.reset': 'latest'}\n",
    "    features_consumer = Consumer(features_consumer_conf)  \n",
    "    \n",
    "    print(f'\\nNow subscribing to features topic:{feature_topic}')\n",
    "        \n",
    "    features_consumer.subscribe([feature_topic])\n",
    "    cnt = 0\n",
    "    msg = None\n",
    "    error_cnt = 0\n",
    "    end_learn_ts = 0\n",
    "    st_learn_ts = 0\n",
    "\n",
    "    st_processing_time = 0\n",
    "    \n",
    "    learning_durations=[]\n",
    "    prediction_durations=[]\n",
    "    processing_durations = []\n",
    "    score_and_truth = []\n",
    "    mem_usage = []\n",
    "    while(True):           \n",
    "        msg = features_consumer.poll(timeout=0.1)    \n",
    "        if msg is None: continue\n",
    "        if msg.error():\n",
    "            error_cnt = error_cnt + 1\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:                    \n",
    "                    if(error_cnt%1000==0):\n",
    "                        print('error')\n",
    "                        print(msg)\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                             (msg.topic(), msg.partition(), msg.offset()))\n",
    "        else:       \n",
    "            try:         \n",
    "                msg_arrival_time = time.time()\n",
    "                message = json.loads(msg.value().decode(\"utf-8\"))            \n",
    "                cnt = cnt + 1\n",
    "                \n",
    "                f = message['f']\n",
    "                y = (message['y']=='true')              \n",
    "                if(cnt==1):\n",
    "                    st_processing_time = time.time()\n",
    "                \n",
    "                st_prediction_time = time.time()            \n",
    "                score = model_artifact.predict_one(f)\n",
    "                score_and_truth.append({'y':y,'score':score})\n",
    "                end_prediction_time = time.time()  \n",
    "                prediction_durations.append(end_prediction_time-st_prediction_time)\n",
    "                \n",
    "                st_learn_ts = time.time()\n",
    "                model_artifact = model_artifact.learn_one(f,y)      \n",
    "                end_learn_ts = time.time()\n",
    "                learning_durations.append(end_learn_ts-st_learn_ts)            \n",
    "                \n",
    "                msg_departure_time = time.time()\n",
    "                processing_durations.append(msg_departure_time-msg_arrival_time)\n",
    "                if(cnt%10==0):\n",
    "                    mem_usage.append(model_artifact._raw_memory_usage)\n",
    "            except Exception as  e:      \n",
    "                print(json.loads(msg.value().decode(\"utf-8\")))\n",
    "                print(e, file=sys.stdout)\n",
    "                ignored = ignored + 1\n",
    "                print(f'ignored ={ignored} total = {cnt}')\n",
    "\n",
    "    print('CLOSING')\n",
    "    features_consumer.commit()\n",
    "    features_consumer.close() \n",
    "    total_time = time.time() - st_processing_time\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30613b73-7440-4d2d-bfdf-792563642547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from river import metrics\n",
    "\n",
    "def print_time_results(durations, type_of_duration):\n",
    "    mean = statistics.mean(durations)\n",
    "    median = statistics.median(durations)\n",
    "    max_dur = max(durations)\n",
    "    min_dur = min(durations)  \n",
    "    print(f'Type of durations : {type_of_duration} ' )\n",
    "    print(f'\\tAVG : {mean}')\n",
    "    print(f'\\MEDIAN : {median}')\n",
    "    print(f'\\MAX : {max_dur}')\n",
    "    print(f'\\MIN : {min_dur}')\n",
    "    \n",
    "def print_results(score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time):\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    for m in score:\n",
    "        y = m['y']\n",
    "        score = m['score']\n",
    "        auc = auc.update(y,score)\n",
    "        f1 = f1.update(y, score)\n",
    "        recall = recall.update(y, score)\n",
    "    \n",
    "  \n",
    "    total_records = len(durations)\n",
    "    avg_memory_usage = statistics.mean(mem_usage)\n",
    "    print(f'Messages consumed:{total_records},Total Cumulative Time: {total_time}')    \n",
    "    print(f'AUC{auc}')\n",
    "    print(f'F1 {f1}')\n",
    "    print(f'RECALL {recall}')\n",
    "    print(f'AVERAGE MEMORY USAGE {avg_memory_usage}')\n",
    "    print_time_results(processing_durations,f'PROCESSING DURATIONS FOR {processing_durations}')\n",
    "    print_time_results(prediction_durations,f'PREDICTION DURATIONS FOR {prediction_durations}')\n",
    "    print_time_results(prediction_durations,f'LEARNING DURATIONS FOR {learning_durations}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444fa65-9fe2-4c14-9134-0974a3c3e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now subscribing to features topic:test_2\n"
     ]
    }
   ],
   "source": [
    "model = ensemble.AdaptiveRandomForestClassifier(seed=8, leaf_prediction=\"mc\")\n",
    "group_id = 'ARFC_8'\n",
    "score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time = consume_messages(group_id,model)\n",
    "print_results(score_and_truth,processing_durations, prediction_durations, learning_durations,mem_usage,total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d489dcf-8cd5-4a4e-9ddc-2cd133a67886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
